# The Iced Americano Index: A Geospatial Economic Analysis of South Korea

This project investigates the "Iced Americano Index," a concept exploring whether the price of a ubiquitous consumer good—the Iced Americano—can serve as an informal economic indicator across different regions of South Korea.

The analysis involves a multi-stage data pipeline:

1. **Data Acquisition**: Scraping cafe locations and menu data from Naver Maps.
2. **Data Processing**: Cleaning and aggregating cafe data, specifically extracting the price of an Americano.
3. **Data Enrichment**: Merging the cafe data with official economic statistics (GRDP and Individual Income) and geospatial boundary data.
4. **Analysis & Visualization**: Performing exploratory data analysis (EDA) to find correlations and creating choropleth maps to visualize findings.


## Project Structure

The repository is organized into a data acquisition pipeline (Python) and an analysis pipeline (R).

### Data Acquisition (Python)

**naver_cafes.py**:
- **Purpose**: Scrapes cafe location data from Naver Maps' internal API.
- **Method**: It uses a grid-based approach. For each target city, it generates a grid of bounding boxes and queries the API for all cafes within each box. This ensures comprehensive coverage of dense urban areas.
- **Features**: Includes checkpointing to resume interrupted scrapes and saves raw API responses for debugging.
- **Output**: A CSV file (`remaining_naver_cafes_api_scraped3.csv`) containing detailed information for thousands of cafes across South Korea.

**naver_menu_scraper.py**:
- **Purpose**: Visits the detail page for each cafe scraped by the first script to extract menu information.
- **Method**: Uses Pyppeteer (a headless browser) to render the JavaScript-heavy pages and extract the HTML from the menu's iframe. It then parses this HTML to find menu items and specifically flags cafes that sell an "아메리카노" (Americano).
- **Features**: Supports batch processing by city and slicing for large cities (e.g., `서울[1000:2000]`). Includes an optional fallback to use OpenAI's GPT for menu extraction if simple parsing fails.
- **Output**: An updated CSV (`scraped_output16.csv`) with new columns: `success` (if menu was scraped), `americano` (TRUE/FALSE), and `menu_items` (a JSON string of the menu).

### Data Analysis (R)

**Cafes_Raw_EDA.R**:
- **Purpose**: A preliminary analysis script to assess the completeness of the location scraping process.
- **Method**: Reads all the raw JSON files saved by `naver_cafes.py` and compares the number of cafes returned (`count`) versus the total number available in that bounding box (`totalCount`).
- **Output**: A choropleth map showing the estimated total number of cafes per province, providing a baseline for market size.

**EDA3.R**:
- **Purpose**: The main analysis and visualization script.
- **Method**:
  - Loads the final scraped data and economic data (GRDP, Individual Income).
  - Parses the JSON menu data to calculate the minimum price of an Americano for each cafe.
  - Aggregates data to the provincial level, calculating mean/median coffee prices.
  - Joins the aggregated data with geospatial shapefiles (from GADM).
  - Generates visualizations: choropleth maps for cafe counts, prices, and affordability indices; histograms and boxplots for price distributions; and scatter plots to show correlations.
  - Calculates correlation coefficients and performs significance tests between coffee prices and economic indicators.
- **Output**: A series of high-quality plots and statistical summaries answering the core research questions.

## Setup and Installation

### Prerequisites
- Git
- Python 3.8+
- R and RStudio

### 1. Clone the Repository
```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```

### 2. Python Environment Setup

It is highly recommended to use a virtual environment.
```bash
# Create a virtual environment
python -m venv venv

# Activate it
# On macOS/Linux:
source venv/bin/activate
# On Windows:
.\venv\Scripts\activate

# Install required Python packages
pip install -r requirements.txt
```

**Note**: You will need to create a `requirements.txt` file with the following content:
```
requests
pandas
pyppeteer
beautifulsoup4
openai
python-dotenv
nest_asyncio
tqdm
```

#### OpenAI API Key (Optional)

The menu scraper has an optional GPT-based fallback. If you wish to use it, create a `.env` file in the root directory:
```
OPENAI_API_KEY="your_openai_api_key_here"
```

### 3. R Environment Setup

Open the project in RStudio. The scripts require several packages. Run the following command in the R console to install them:
```r
install.packages(c("readr", "dplyr", "sf", "geodata", "ggplot2", "viridis", "jsonlite", "purrr", "stringr", "extrafont", "showtext", "gridExtra", "reshape2"))
```

#### Font Setup for Korean Characters (Important for R plots)

For Korean text to display correctly in the plots generated by `EDA3.R`, you may need to install and set up a Korean font like Nanum.
```r
# Run this once in R to install the font package
install.packages("showtext")
library(showtext)

# This will download and register the Nanum Gothic font with R
font_add_google("Nanum Gothic", "nanumgothic")

# Call this at the start of your session or script to enable showtext rendering
showtext_auto()
```

## How to Run the Pipeline

Follow these steps in order.

### Step 1: Scrape Cafe Locations

Modify the `CITY_CENTERS` and `CITIES_TO_SCRAPE` dictionaries in `naver_cafes.py` to select your target cities. Then run the script from your terminal:
```bash
python naver_cafes.py
```

This will produce `../data/remaining_naver_cafes_api_scraped3.csv` and populate the `../data/raw_api_responses` directory.

### Step 2: Scrape Cafe Menus

Modify the `cities` list in `naver_menu_scraper.py` to match the cities you scraped in Step 1. You can use slicing (e.g., `'서울[0:1000]'`) to break up large cities into smaller, manageable chunks.
```bash
python naver_menu_scraper.py
```

This will update the cafe data and save it as `../data/scraped_output16.csv`. You may need to combine this file with previous outputs if you run the scraper in multiple batches.

### Step 3: Run the Analysis

1. **Combine Data**: Ensure you have a final, combined CSV of all scraped menu data. In `EDA3.R`, update the filename in `read_csv()` to point to this file (e.g., `combined_scraped_output.csv`).
2. **Set Working Directory**: Open RStudio and set the project's root folder as the working directory.
3. **Run Scripts**:
   - First, run `Cafes_Raw_EDA.R` to get an overview of the scraper's coverage.
   - Then, run `EDA3.R` to perform the full analysis and generate all maps and plots. The outputs will be displayed in the RStudio 'Plots' pane.

## Data Sources

- **Cafe Data**: Scraped from Naver Maps (© NAVER Corp).
- **Economic Data**:
  - GRDP per Capita (2022): Statistics Korea (KOSIS).
  - Individual Income per Capita (2021): Statistics Korea (KOSIS).
- **Geospatial Data**: Administrative boundaries from the GADM database.